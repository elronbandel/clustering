{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-21 13:44:21--  https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3347213 (3.2M) [application/x-httpd-php]\n",
      "Saving to: ‘dataset_diabetes.zip’\n",
      "\n",
      "100%[======================================>] 3,347,213   2.14MB/s   in 1.5s   \n",
      "\n",
      "2020-12-21 13:44:24 (2.14 MB/s) - ‘dataset_diabetes.zip’ saved [3347213/3347213]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00296/dataset_diabetes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  dataset_diabetes.zip\n",
      "  inflating: dataset_diabetes/diabetic_data.csv  \n",
      "  inflating: dataset_diabetes/IDs_mapping.csv  \n"
     ]
    }
   ],
   "source": [
    "!unzip dataset_diabetes.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== HEADER ===========\n",
      "Readme.txt for \"Fifty Victorian Era Novelists Authorship Attribution Data\"\n",
      "Item Handle: https://dataworks.iupui.edu/handle/11243/23\n",
      "DOI: http://dx.doi.org/10.7912/D2N65J\n",
      "Documentation written on 20180517, 20180601\n",
      "By Heather Coates, DataWorks Repository Manager\n",
      "Revised by Abdulmecit Gungor, investigator\n",
      "\n",
      "=========== SUGGESTED DATA CITATION ===========\n",
      "\n",
      "Please cite this data in the references for any publication which uses it. \n",
      "Gungor, A. (2018). Fifty Victorian Era Novelists Authorship Attribution Data. IUPUI University Library. http://dx.doi.org/10.7912/D2N65J\n",
      "\n",
      "\n",
      "=========== PRIMARY STUDY INFORMATION ===========\n",
      "ACKNOWLEDGEMENTS\n",
      "Project title: Fifty Victorian Era Novelists Authorship Attribution Data\n",
      "Investigator Name: Abdulmecit Gungor\n",
      "Investigator Institution: Indiana University Purdue University Indianapolis (IUPUI) School of Science, Department of Computer & Information Science\n",
      "Investigator Email: mgungor@iu.edu\n",
      "Investigator Role (related to this dataset): graduate student investigator\n",
      "\n",
      "DATA SOURCE\n",
      "Google Big Query https://cloud.google.com/bigquery/public-data/gdelt-books\n",
      "The data was extracted through https://blog.gdeltproject.org/ using Google Big Query. This dataset is publicly available for anyone to use under the following terms provided by the Dataset Source �http://gdeltproject.org/about.html. The GDELT Project is an open platform for research and analysis of global society and thus all datasets released by the GDELT Project are available for unlimited and unrestricted use for any academic, commercial, or governmental use of any kind without fee.\n",
      "\n",
      "DATA PREPARATION\n",
      "Workflow at https://github.com/agungor2/Authorship_Attribution\n",
      "Data were prepared for processing using the procedures described in the thesis \"Benchmarking Authorship Attribution Over a Thousand Books By Victorian Era Authors\", Section 3.1 \n",
      "\n",
      "DATE(S) of DATA COLLECTION\n",
      "Data were retrieved November, 2016\n",
      "\n",
      "FILE DESCRIPTION: DATA DICTIONARY\n",
      "Name, Size, Bytes, Class\n",
      "WW, 50x3500, 1400000, double\n",
      "aid, 93600x1, 748800, double\n",
      "bid, 93600x1, 748800, double\n",
      "ind, 93600x1, 748800, double\n",
      "shortened_vocab, 1x10000, 1254644, cell\n",
      "test_ind, 93600x1, 93600, logical\n",
      "tfidf, 1113x50920, 453391680, double\n",
      "train_ind, 93600x1, 93600, logical\n",
      "txt_pieces, 93600x1000, 748800000, double\n",
      "vocab, 1x50920, 6387934, cell\n",
      "\n",
      "VARIABLE NAMING CONVENTIONS\n",
      "Name, Description\n",
      "WW, Author Word list\n",
      "aid, Author Id\n",
      "bid, Book Id\n",
      "ind, Index numbers\n",
      "shortened_vocab, 10000 Vocabulary list\n",
      "test_ind, Testing Indexes\n",
      "tfidf, Tfidf Scores\n",
      "train_ind, Training Indexes\n",
      "txt_pieces, All one hot encoded data\n",
      "vocab, All vocabulary list\n",
      "\n",
      "\n",
      "=========== THESIS PROJECT ===========\n",
      "ABSTRACT\n",
      "Authorship attribution is the process of identifying the author of a given text and from the machine learning perspective, it can be seen as a classification problem. To create the largest publicly available authorship attribution dataset we've extracted the works of 50 well-known Victorian-era authors. All of these extracted works are novels. In order to create non-exhaustive learning problem, we've provided 45 authors in training and 50 authors in the testing data. 5 missing authors in testing consist of %34 of all testing set. Each instance then represented with a 1000 word pieces for each author. There are 93600 text piece instance in total each which consist of 1000 words. To make the problem a bit more challenging, we've separated different books for both training and testing.  We have performed 5 main feature extraction technique on this data and compared the performance of such features within different classifiers and deep learning structures. The usage of Word2Vec in authorship attribution problem is also introduced with two main approaches: author based Word2Vec training and treating each author's text pieces individually. Support vector machine classifiers with nu-SVC type is observed to give best success rates on the stacked useful feature set. \n",
      "\n",
      "RESEARCH AIMS\n",
      "The main purpose of this work is to lay the foundations of feature extraction techniques in Authorship Attribution problems. These are lexical, character-level, syntactic, semantic, application specific features. In order to showcase each of these feature extraction techniques  we have aimed to offer a new data resource for the author attribution research community and demonstrated them with examples. These examples can be found through https://github.com/agungor2/Authorship_Attribution. The dataset we have introduced consists of works of Victorian era authors and the main feature extraction techniques.\n",
      " \n",
      "METHODS\n",
      "To decrease the bias and create a reliable authorship attribution dataset the following criteria have been chosen to filter out authors in Gdelt database: English language writing authors, authors that have enough books available (at least 5), 19th century authors. With these criteria 50 authors have been selected and their books were queried through Big Query Gdelt database. The next task has been cleaning the dataset due to OCR reading problems in the original raw form. To achieve that, firstly all books have been scanned through to get the overall number of unique words and each words frequencies. While scanning the texts, the first 500 words and the last 500 words have been removed to take out specific features such as the name of the author, the name of the book and other word specific features that could make the classification task easier. After this step, we have chosen top 10,000 words that occurred in the whole 50 authors text data corpus. The words that are not in top 10,000 words were removed while keeping the rest of the sentence structure intact. Afterwards, the words are represented with numbers from 1 to 10,000 reverse ordered according to their frequencies. The entire book is split into text fragments with 1000 words each. We separately maintained author and book identification number for each one of them in different arrays. Text segments with less than 1000 words were filled with zeros to keep them in the dataset as well. 1000 words make approximately 2 pages of writing, which is long enough to extract a variety of features from the document. The reason why we have represented top 10,000 words with numbers is to keep the anonymity of texts and allow researchers to run feature extraction techniques faster. Dealing with large amounts of text data can be more challenging than numerical data for some feature extraction techniques.\n",
      "\n",
      "\n",
      "=========== CREDITS ===========\n",
      "Template provided by Indiana University UITS Research Storage, Indiana University Bloomington Libraries, IUPUI University Library\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat dataset/Gungor_2018_VictorianAuthorAttribution_readme.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-12-20 16:14:50--  https://archive.ics.uci.edu/ml/machine-learning-databases/00454/Data%20Description.pdf\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 92295 (90K) [application/x-httpd-php]\n",
      "Saving to: ‘Data Description.pdf’\n",
      "\n",
      "100%[======================================>] 92,295       239KB/s   in 0.4s   \n",
      "\n",
      "2020-12-20 16:14:51 (239 KB/s) - ‘Data Description.pdf’ saved [92295/92295]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/00454/Data%20Description.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
